---
title: "test 3: W2V vs BERT - word to def"
output:
  html_notebook: default
  pdf_document: default
---


```{r, echo = FALSE}
library("reticulate")
library("data.table")

path_root_proj <- "/home/sergio/sync/projects/proj-nlp/"
path_conda_exe <- "/opt/anaconda3/condabin/conda" 

# loading data
FILE_PY_READ_PICKLE <- paste0(path_root_proj, "app/lib/py/reader/read_pickle.py")
#conda_list(conda = path_conda_exe)
#use_condaenv("nlpenv", conda = path_conda_exe)
#py_config()
source_python(FILE_PY_READ_PICKLE)

FILE_PKL_TO_READ_SIM <- paste0(path_root_proj, "data/exchange/ws353_input_sim")
FILE_PKL_TO_READ_W2V_WORDS <- paste0(path_root_proj, "data/exchange/ws353_w2v_words_context")
FILE_PKL_TO_READ_W2V_DEF <- paste0(path_root_proj, "data/exchange/ws353_w2v_def_cambridge")

#### read dataframe from .pkl file
data_sim <- setDT(read_pickle_file(FILE_PKL_TO_READ_SIM))
data_w2v_words <- setDT(read_pickle_file(FILE_PKL_TO_READ_W2V_WORDS))
data_w2v_def <- setDT(read_pickle_file(FILE_PKL_TO_READ_W2V_DEF))

review_cols <- c("id", "w", "id_token", "token")

```

### DATA

dataset with mannual annotation of similaraties between pairs of words (wordsim353);


```{r}
head(data_sim)
```


dataset with vector representations of the set of words appeared;

```{r}
head(data_w2v_words)
```

we select rows with equal w and token 

NOTE: w2v is context-free but we could test it.


```{r echo=FALSE}
length(unique(data_w2v_words$w))

data_w2v_words <- subset(data_w2v_words[w == token], select = -c(id_token, token))
```

and dataset with words and its definitions,

```{r echo = FALSE}
head(data_w2v_def, 10)
```


### DEFINITIONS TREATMENT

Firstly, we compute sum and average of words that compound definition.

```{r echo = FALSE}
data_w2v_def <- subset(data_w2v_def, select = -c(id_token, token))
w2v_vector_cols <- colnames(data_w2v_def)[3:ncol(data_w2v_def)]

w2v_def_sum <- data.frame()
w2v_def_avg <- data.frame()

for(id_w in sort(unique(data_w2v_def$id))){
  data_w <- data_w2v_def[id == id_w]
  
  vec_sum <- as.data.frame(c(unique(data_w[ , .(id, w), ]), sapply(w2v_vector_cols, function(c){sum(data_w[[c]])})))
  vec_avg <- as.data.frame(c(unique(data_w[ , .(id, w), ]), sapply(w2v_vector_cols, function(c){mean(data_w[[c]])})))
  
  w2v_def_sum <- rbind(w2v_def_sum, vec_sum)
  w2v_def_avg <- rbind(w2v_def_avg, vec_avg)
}

saveRDS(w2v_def_sum, paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/w2v_def_sum.rds"))
saveRDS(w2v_def_avg, paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/w2v_def_avg.rds"))

head(w2v_def_sum)
head(w2v_def_avg)
```








```{r}
get_vector_of_w <- function(row = 1, col_vec_start = 3, col_vec_end = 302){
    return (sapply(col_vec_start:col_vec_end, function(d){as.numeric(df_vec[row, c(colnames(df_vec)[d]), with = FALSE])}))
}

compute_cosin <- function(a, b){
    cos_theta <- cos( sum(a*b) / ( sqrt(sum(a * a)) * sqrt(sum(b * b)) ) )
    #theta <- acos( sum(a*b) / ( sqrt(sum(a * a)) * sqrt(sum(b * b)) ) )
    return (cos_theta)
}


vector_cosines <- sapply(1:nrow(data_sim), function(r){
    word1 <- as.character(data_sim[r, .(w1), ])
    word2 <- as.character(data_sim[r, .(w2), ])
    
    index_word1 <- which(df_vec$w == word1)
    index_word2 <- which(df_vec$w == word2)
    
    rep1 <- get_vector_of_w(row = index_word1)
    rep2 <- get_vector_of_w(row = index_word2)
    
    cosine <- compute_cosin(rep1, rep2)
    print(paste0("computing words - ", word1, " and ", word2, " - ", cosine))
    
    return (cosine)
})

```



