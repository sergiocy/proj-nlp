---
title: "test 3: W2V vs BERT - word to def"
output:
  html_notebook: default
  pdf_document: default
---


```{r, echo = FALSE}
library("reticulate")
library("data.table")

#path_root_proj <- "/home/s/sync/projects/proj-nlp/"
path_root_proj <- "C:/Users/scordoba/sync/projects/proj-nlp/"
#path_conda_exe <- "/opt/anaconda3/condabin/conda" 

# loading data
FILE_PY_READ_PICKLE <- paste0(path_root_proj, "app/lib/py/reader/read_pickle.py")
#conda_list(conda = path_conda_exe)
#use_condaenv("nlpenv", conda = path_conda_exe)
#py_config()
source_python(FILE_PY_READ_PICKLE)

FILE_PKL_TO_READ_SIM <- paste0(path_root_proj, "data/exchange/ws353_input_sim")
FILE_PKL_TO_READ_W2V_WORDS <- paste0(path_root_proj, "data/exchange/ws353_w2v_words_context")
FILE_PKL_TO_READ_W2V_DEF <- paste0(path_root_proj, "data/exchange/ws353_w2v_def_cambridge")

#### read dataframe from .pkl file
data_sim <- setDT(read_pickle_file(FILE_PKL_TO_READ_SIM))
data_w2v_words <- setDT(read_pickle_file(FILE_PKL_TO_READ_W2V_WORDS))
data_w2v_def <- setDT(read_pickle_file(FILE_PKL_TO_READ_W2V_DEF))

review_cols <- c("id", "w", "id_token", "token")

```

### DATA

dataset with mannual annotation of similaraties between pairs of words (wordsim353);


```{r}
head(data_sim)
```


dataset with vector representations of the set of words appeared;

```{r}
head(data_w2v_words)
```

we select rows with equal w and token 

NOTE: w2v is context-free but we could test it.


```{r echo=FALSE}
length(unique(data_w2v_words$w))

data_w2v_words <- subset(data_w2v_words[w == token], select = -c(id_token, token))
```

and dataset with words and its definitions,

```{r echo = FALSE}
head(data_w2v_def, 10)
```


### DEFINITIONS TREATMENT

Firstly, we compute sum and average of words that compound definition.

```{r echo = FALSE}
data_w2v_def <- subset(data_w2v_def, select = -c(id_token, token))
w2v_vector_cols <- colnames(data_w2v_def)[3:ncol(data_w2v_def)]

w2v_def_sum <- data.frame()
w2v_def_avg <- data.frame()

for(id_w in sort(unique(data_w2v_def$id))){
  data_w <- data_w2v_def[id == id_w]
  
  vec_sum <- as.data.frame(c(unique(data_w[ , .(id, w), ]), sapply(w2v_vector_cols, function(c){sum(data_w[[c]])})))
  vec_avg <- as.data.frame(c(unique(data_w[ , .(id, w), ]), sapply(w2v_vector_cols, function(c){mean(data_w[[c]])})))
  
  w2v_def_sum <- rbind(w2v_def_sum, vec_sum)
  w2v_def_avg <- rbind(w2v_def_avg, vec_avg)
}

saveRDS(w2v_def_sum, paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/w2v_def_sum.rds"))
saveRDS(w2v_def_avg, paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/w2v_def_avg.rds"))

head(w2v_def_sum)
head(w2v_def_avg)
```


```{r echo = FALSE}
df_comp <- w2v_def_sum
head(df_comp)
dim(df_comp)
df_w <- data_w2v_words[, , ]
head(df_w)
dim(df_w)


# ..we select words with definition...
df_w <- df_w[df_w$w %in% df_comp$w] 



get_vector_of_w <- function(df_vec, row = 1, col_vec_start = 3, col_vec_end = 302){
    return (sapply(col_vec_start:col_vec_end, function(d){as.numeric(df_vec[row, c(colnames(df_vec)[d]), with = FALSE])}))
}


compute_cosin <- function(a, b){
    cos_theta <- ( sum(a*b) / ( sqrt(sum(a * a)) * sqrt(sum(b * b)) ) )
    #theta <- acos( sum(a*b) / ( sqrt(sum(a * a)) * sqrt(sum(b * b)) ) )
    return (cos_theta)
}


#i_word <- 1 
similarities <- lapply(1:nrow(df_w), function(i_word){
    vec_w <- get_vector_of_w(df_w, row = i_word, col_vec_start = 3, col_vec_end = 302)
    df_sim_one_word <- c(df_w[i_word, , ]$id, df_w[i_word, , ]$w
                        , sapply(1:nrow(df_comp), function(i_comp){
                                                        vec_comp <- get_vector_of_w(df_comp, row = i_comp, col_vec_start = 3, col_vec_end = 302)
                                                        compute_cosin(vec_w, vec_comp)
                                                    }
                                ) 
                        )
    df_sim_one_word <- as.data.frame(matrix(df_sim_one_word, nrow = 1, ncol = length(df_sim_one_word)))
    colnames(df_sim_one_word) <- c("id", "w", sapply(1:nrow(df_comp), function(i_comp){paste0("def_", df_comp[i_comp, , ]$w)}))
    
    print(paste0(Sys.time(), " - computed ", i_word, " word: ", df_w[i_word, , ]$w))
    
    return(df_sim_one_word)
})


similarities <- rbindlist(similarities)
print(similarities)

saveRDS(similarities, paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/ws353_w2v_similarities_w_to_def_cambridge.rds"))

#v1 <- df_w[1, 3:302] 
#v2 <- df_comp[1, 3:302]
#compute_cosin(v1, v2)
```






```{r}


vector_cosines <- sapply(1:nrow(data_sim), function(r){
    word1 <- as.character(data_sim[r, .(w1), ])
    word2 <- as.character(data_sim[r, .(w2), ])
    
    index_word1 <- which(df_vec$w == word1)
    index_word2 <- which(df_vec$w == word2)
    
    rep1 <- get_vector_of_w(row = index_word1)
    rep2 <- get_vector_of_w(row = index_word2)
    
    cosine <- compute_cosin(rep1, rep2)
    print(paste0("computing words - ", word1, " and ", word2, " - ", cosine))
    
    return (cosine)
})

```



