<<<<<<< HEAD
<<<<<<< HEAD
=======
---
title: "test 3: W2V vs BERT - ranking"
output:
  pdf_document: default
  html_notebook: default
---


```{r, echo = FALSE}
library("reticulate")
library("data.table")
library("stringr")
library("ggplot2")
source("../fun_compute_composition.R")
source("../fun_compute_similarities_matrix.R")

#path_root_proj <- "/home/s/sync/projects/proj-nlp/"
path_root_proj <- "C:/Users/scordoba/sync/projects/proj-nlp/"
#path_conda_exe <- "/opt/anaconda3/condabin/conda" 

w2v_def_avg_ranking <- readRDS(paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/w2v_def_avg_ranking.rds"))
w2v_def_sum_ranking <- readRDS(paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/w2v_def_sum_ranking.rds"))
bert_def_avg_ranking <- readRDS(paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/bert_def_avg_ranking.rds"))
bert_def_sum_ranking <- readRDS(paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/bert_def_sum_ranking.rds"))
```

### DATA

Similarities ranking between words and definitions using average and sum as composition functions for w2v and BERT-free-context.

As example we will show w2v.


```{r}
# w2v ranking using average as composition function
head(w2v_def_avg_ranking[[1]][ , 1:4])
```

```{r}
# w2v ranking similarities using average as composition function
head(w2v_def_avg_ranking[[2]][ , 1:4])
```

```{r}
# w2v ranking using average as composition function
head(w2v_def_sum_ranking[[1]][ , 1:4])
```

```{r}
# w2v ranking similarities using average as composition function
head(w2v_def_sum_ranking[[2]][ , 1:4])
```


Analogous for BERT representation.



NOTE: the same cosine similarities for average and sum!!!! ...but really the compound vector is different 



### RANKING

We study this rankings

#### w2v compound using words definition average

We can plot the average of similarities with maximum and minimum values,


```{r, echo = FALSE}
sim_words <- w2v_def_avg_ranking[[1]]
sim_scores <- w2v_def_avg_ranking[[2]]

sim_review <- data.frame(
                    x = 2:ncol(sim_scores),
                    average = sapply(2:ncol(sim_scores), function(i){mean(as.numeric(sim_scores[[i]]))}),
                    maximum = sapply(2:ncol(sim_scores), function(i){max(as.numeric(sim_scores[[i]]))}),
                    minimum = sapply(2:ncol(sim_scores), function(i){min(as.numeric(sim_scores[[i]]))})
                    )

ggplot(sim_review, aes(x=x)) + 
  geom_line(aes(y = average), color = "darkred") + 
  geom_line(aes(y = maximum), color="steelblue", linetype="twodash") + 
  geom_line(aes(y = minimum), color="steelblue", linetype="twodash") 
```

We can observe that there is similarities equal to one. We review words with simialrity equal to one.

```{r}
sim_scores[as.numeric(sim_scores[["1"]]) == 1, 1:4]
```

```{r}
sim_words[w %in% c("car", "calculation", "dollar"), 1:4]
```


And we can count the number of definitions equal to word in each ranking position,


```{r, echo = FALSE}
qplot( 2:ncol(sim_scores), sapply(2:ncol(sim_scores)
                                  , function(i){nrow(sim_words[w == sim_words[[i]]])}) , xlab = "position", ylab = "coincidences")
```



#### BERT (free-context) compound using words definition average


```{r, echo = FALSE}
sim_words <- bert_def_avg_ranking[[1]]
sim_scores <- bert_def_avg_ranking[[2]]

sim_review <- data.frame(
                    x = 2:ncol(sim_scores),
                    average = sapply(2:ncol(sim_scores), function(i){mean(as.numeric(sim_scores[[i]]))}),
                    maximum = sapply(2:ncol(sim_scores), function(i){max(as.numeric(sim_scores[[i]]))}),
                    minimum = sapply(2:ncol(sim_scores), function(i){min(as.numeric(sim_scores[[i]]))})
                    )

ggplot(sim_review, aes(x=x)) + 
  geom_line(aes(y = average), color = "darkred") + 
  geom_line(aes(y = maximum), color="steelblue", linetype="twodash") + 
  geom_line(aes(y = minimum), color="steelblue", linetype="twodash") 
```

We can observe similarity values (we remember that we are using the cosine similarity) highest than w2v. In the same way, we can review words with similarity equal to one,

```{r}
sim_scores[as.numeric(sim_scores[["1"]]) == 1, 1:4]
```

And we can count the number of definitions equal to word in each ranking position,


```{r, echo = FALSE}
qplot( 2:ncol(sim_scores), sapply(2:ncol(sim_scores)
                                  , function(i){nrow(sim_words[w == sim_words[[i]]])}) , xlab = "position", ylab = "coincidences")
```

We observe a different behaviour in w2v and BERT. With w2v we observe that the first positions (30 - 50) in the ranking acummulate o lot of correct definitions, nevertheless, BERT looks ñike more distributed. 

In this point the text used to training each model can take importance.
=======
>>>>>>> fcadabf90f0f8b33e1eacca2e7ffae1cfcf92c89
---
title: "test 3: W2V vs BERT - ranking"
output:
  pdf_document: default
  html_notebook: default
---


```{r, echo = FALSE}
library("reticulate")
library("data.table")
library("stringr")
library("ggplot2")
source("../fun_compute_composition.R")
source("../fun_compute_similarities_matrix.R")

#path_root_proj <- "/home/s/sync/projects/proj-nlp/"
path_root_proj <- "C:/Users/scordoba/sync/projects/proj-nlp/"
#path_conda_exe <- "/opt/anaconda3/condabin/conda" 

w2v_def_avg_ranking <- readRDS(paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/w2v_def_avg_ranking.rds"))
w2v_def_sum_ranking <- readRDS(paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/w2v_def_sum_ranking.rds"))
bert_def_avg_ranking <- readRDS(paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/bert_def_avg_ranking.rds"))
bert_def_sum_ranking <- readRDS(paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/bert_def_sum_ranking.rds"))
```

### DATA

Similarities ranking between words and definitions using average and sum as composition functions for w2v and BERT-free-context.

As example we will show w2v.


```{r}
# w2v ranking using average as composition function
head(w2v_def_avg_ranking[[1]][ , 1:4])
```

```{r}
# w2v ranking similarities using average as composition function
head(w2v_def_avg_ranking[[2]][ , 1:4])
```

```{r}
# w2v ranking using average as composition function
head(w2v_def_sum_ranking[[1]][ , 1:4])
```

```{r}
# w2v ranking similarities using average as composition function
head(w2v_def_sum_ranking[[2]][ , 1:4])
```


Analogous for BERT representation.



NOTE: the same cosine similarities for average and sum!!!! ...but really the compound vector is different 



### RANKING

We study this rankings

#### w2v compound using words definition average

We can plot the average of similarities with maximum and minimum values,


```{r, echo = FALSE}
sim_words <- w2v_def_avg_ranking[[1]]
sim_scores <- w2v_def_avg_ranking[[2]]

sim_review <- data.frame(
                    x = 2:ncol(sim_scores),
                    average = sapply(2:ncol(sim_scores), function(i){mean(as.numeric(sim_scores[[i]]))}),
                    maximum = sapply(2:ncol(sim_scores), function(i){max(as.numeric(sim_scores[[i]]))}),
                    minimum = sapply(2:ncol(sim_scores), function(i){min(as.numeric(sim_scores[[i]]))})
                    )

ggplot(sim_review, aes(x=x)) + 
  geom_line(aes(y = average), color = "darkred") + 
  geom_line(aes(y = maximum), color="steelblue", linetype="twodash") + 
  geom_line(aes(y = minimum), color="steelblue", linetype="twodash") 
```

We can observe that there is similarities equal to one. We review words with simialrity equal to one.

```{r}
sim_scores[as.numeric(sim_scores[["1"]]) == 1, 1:4]
```

```{r}
sim_words[w %in% c("car", "calculation", "dollar"), 1:4]
```


And we can count the number of definitions equal to word in each ranking position,


```{r, echo = FALSE}
qplot( 2:ncol(sim_scores), sapply(2:ncol(sim_scores)
                                  , function(i){nrow(sim_words[w == sim_words[[i]]])}) , xlab = "position", ylab = "coincidences")
```



#### BERT (free-context) compound using words definition average


```{r, echo = FALSE}
sim_words <- bert_def_avg_ranking[[1]]
sim_scores <- bert_def_avg_ranking[[2]]

sim_review <- data.frame(
                    x = 2:ncol(sim_scores),
                    average = sapply(2:ncol(sim_scores), function(i){mean(as.numeric(sim_scores[[i]]))}),
                    maximum = sapply(2:ncol(sim_scores), function(i){max(as.numeric(sim_scores[[i]]))}),
                    minimum = sapply(2:ncol(sim_scores), function(i){min(as.numeric(sim_scores[[i]]))})
                    )

ggplot(sim_review, aes(x=x)) + 
  geom_line(aes(y = average), color = "darkred") + 
  geom_line(aes(y = maximum), color="steelblue", linetype="twodash") + 
  geom_line(aes(y = minimum), color="steelblue", linetype="twodash") 
```

We can observe similarity values (we remember that we are using the cosine similarity) highest than w2v. In the same way, we can review words with similarity equal to one,

```{r}
sim_scores[as.numeric(sim_scores[["1"]]) == 1, 1:4]
```

And we can count the number of definitions equal to word in each ranking position,


```{r, echo = FALSE}
qplot( 2:ncol(sim_scores), sapply(2:ncol(sim_scores)
                                  , function(i){nrow(sim_words[w == sim_words[[i]]])}) , xlab = "position", ylab = "coincidences")
```

We observe a different behaviour in w2v and BERT. With w2v we observe that the first positions (30 - 50) in the ranking acummulate o lot of correct definitions, nevertheless, BERT looks ñike more distributed. 

In this point the text used to training each model can take importance.
<<<<<<< HEAD
=======
---
title: "test 3: W2V vs BERT - ranking"
output:
  pdf_document: default
  html_notebook: default
---


```{r, echo = FALSE}
library("reticulate")
library("data.table")
library("stringr")
library("ggplot2")
source("../fun_compute_composition.R")
source("../fun_compute_similarities_matrix.R")

#path_root_proj <- "/home/s/sync/projects/proj-nlp/"
path_root_proj <- "C:/Users/scordoba/sync/projects/proj-nlp/"
#path_conda_exe <- "/opt/anaconda3/condabin/conda" 

w2v_def_avg_ranking <- readRDS(paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/w2v_def_avg_ranking.rds"))
w2v_def_sum_ranking <- readRDS(paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/w2v_def_sum_ranking.rds"))
bert_def_avg_ranking <- readRDS(paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/bert_def_avg_ranking.rds"))
bert_def_sum_ranking <- readRDS(paste0(path_root_proj, "data/exchange/20200118-test3-wToDef/bert_def_sum_ranking.rds"))
```

### DATA

Similarities ranking between words and definitions using average and sum as composition functions for w2v and BERT-free-context.

As example we will show w2v.


```{r}
# w2v ranking using average as composition function
head(w2v_def_avg_ranking[[1]][ , 1:4])
```

```{r}
# w2v ranking similarities using average as composition function
head(w2v_def_avg_ranking[[2]][ , 1:4])
```

```{r}
# w2v ranking using average as composition function
head(w2v_def_sum_ranking[[1]][ , 1:4])
```

```{r}
# w2v ranking similarities using average as composition function
head(w2v_def_sum_ranking[[2]][ , 1:4])
```


Analogous for BERT representation.



NOTE: the same cosine similarities for average and sum!!!! ...but really the compound vector is different 



### RANKING

We study this rankings

#### w2v compound using words definition average

We can plot the average of similarities with maximum and minimum values,


```{r, echo = FALSE}
sim_words <- w2v_def_avg_ranking[[1]]
sim_scores <- w2v_def_avg_ranking[[2]]

sim_review <- data.frame(
                    x = 2:ncol(sim_scores),
                    average = sapply(2:ncol(sim_scores), function(i){mean(as.numeric(sim_scores[[i]]))}),
                    maximum = sapply(2:ncol(sim_scores), function(i){max(as.numeric(sim_scores[[i]]))}),
                    minimum = sapply(2:ncol(sim_scores), function(i){min(as.numeric(sim_scores[[i]]))})
                    )

ggplot(sim_review, aes(x=x)) + 
  geom_line(aes(y = average), color = "darkred") + 
  geom_line(aes(y = maximum), color="steelblue", linetype="twodash") + 
  geom_line(aes(y = minimum), color="steelblue", linetype="twodash") 
```

We can observe that there is similarities equal to one. We review words with simialrity equal to one.

```{r}
sim_scores[as.numeric(sim_scores[["1"]]) == 1, 1:4]
```

```{r}
sim_words[w %in% c("car", "calculation", "dollar"), 1:4]
```


And we can count the number of definitions equal to word in each ranking position,


```{r, echo = FALSE}
qplot( 2:ncol(sim_scores), sapply(2:ncol(sim_scores)
                                  , function(i){nrow(sim_words[w == sim_words[[i]]])}) , xlab = "position", ylab = "coincidences")
```



#### BERT (free-context) compound using words definition average


```{r, echo = FALSE}
sim_words <- bert_def_avg_ranking[[1]]
sim_scores <- bert_def_avg_ranking[[2]]

sim_review <- data.frame(
                    x = 2:ncol(sim_scores),
                    average = sapply(2:ncol(sim_scores), function(i){mean(as.numeric(sim_scores[[i]]))}),
                    maximum = sapply(2:ncol(sim_scores), function(i){max(as.numeric(sim_scores[[i]]))}),
                    minimum = sapply(2:ncol(sim_scores), function(i){min(as.numeric(sim_scores[[i]]))})
                    )

ggplot(sim_review, aes(x=x)) + 
  geom_line(aes(y = average), color = "darkred") + 
  geom_line(aes(y = maximum), color="steelblue", linetype="twodash") + 
  geom_line(aes(y = minimum), color="steelblue", linetype="twodash") 
```

We can observe similarity values (we remember that we are using the cosine similarity) highest than w2v. In the same way, we can review words with similarity equal to one,

```{r}
sim_scores[as.numeric(sim_scores[["1"]]) == 1, 1:4]
```

And we can count the number of definitions equal to word in each ranking position,


```{r, echo = FALSE}
qplot( 2:ncol(sim_scores), sapply(2:ncol(sim_scores)
                                  , function(i){nrow(sim_words[w == sim_words[[i]]])}) , xlab = "position", ylab = "coincidences")
```

We observe a different behaviour in w2v and BERT. With w2v we observe that the first positions (30 - 50) in the ranking acummulate o lot of correct definitions, nevertheless, BERT looks ñike more distributed. 

In this point the text used to training each model can take importance.
=======
>>>>>>> fcadabf90f0f8b33e1eacca2e7ffae1cfcf92c89
>>>>>>> c1f6d30eb090456dd5db1cd92d3ef30e07111bac
